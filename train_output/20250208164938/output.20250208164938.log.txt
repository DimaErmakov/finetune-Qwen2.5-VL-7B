2025-02-08 16:50:43-finetune_distributed.py:246-INFO >> Batch 1 of epoch 1/10, training loss : 0.7014365792
2025-02-08 16:51:05-finetune_distributed.py:246-INFO >> Batch 2 of epoch 1/10, training loss : 0.6006619334
2025-02-08 16:51:12-finetune_distributed.py:246-INFO >> Batch 1 of epoch 2/10, training loss : 0.5537893772
2025-02-08 16:51:30-finetune_distributed.py:246-INFO >> Batch 2 of epoch 2/10, training loss : 0.2110770941
2025-02-08 16:51:38-finetune_distributed.py:246-INFO >> Batch 1 of epoch 3/10, training loss : 0.2664038539
2025-02-08 16:51:54-finetune_distributed.py:246-INFO >> Batch 2 of epoch 3/10, training loss : 0.0232144538
2025-02-08 16:52:01-finetune_distributed.py:246-INFO >> Batch 1 of epoch 4/10, training loss : 0.1641263664
2025-02-08 16:52:18-finetune_distributed.py:246-INFO >> Batch 2 of epoch 4/10, training loss : 0.0092941979
2025-02-08 16:52:25-finetune_distributed.py:246-INFO >> Batch 1 of epoch 5/10, training loss : 0.0274769347
2025-02-08 16:52:43-finetune_distributed.py:246-INFO >> Batch 2 of epoch 5/10, training loss : 0.0002155229
2025-02-08 16:52:50-finetune_distributed.py:246-INFO >> Batch 1 of epoch 6/10, training loss : 0.0094001526
2025-02-08 16:53:07-finetune_distributed.py:246-INFO >> Batch 2 of epoch 6/10, training loss : 0.0000878376
2025-02-08 16:53:14-finetune_distributed.py:246-INFO >> Batch 1 of epoch 7/10, training loss : 0.0044305990
2025-02-08 16:53:31-finetune_distributed.py:246-INFO >> Batch 2 of epoch 7/10, training loss : 0.0000352633
2025-02-08 16:53:38-finetune_distributed.py:246-INFO >> Batch 1 of epoch 8/10, training loss : 0.0002418581
2025-02-08 16:53:56-finetune_distributed.py:246-INFO >> Batch 2 of epoch 8/10, training loss : 0.0000051071
2025-02-08 16:54:03-finetune_distributed.py:246-INFO >> Batch 1 of epoch 9/10, training loss : 0.0001361915
2025-02-08 16:54:21-finetune_distributed.py:246-INFO >> Batch 2 of epoch 9/10, training loss : 0.0000053016
2025-02-08 16:54:28-finetune_distributed.py:246-INFO >> Batch 1 of epoch 10/10, training loss : 0.0000987772
2025-02-08 16:54:47-finetune_distributed.py:246-INFO >> Batch 2 of epoch 10/10, training loss : 0.0000080494
2025-02-08 16:55:10-finetune_distributed.py:186-INFO >> chat template saved in train_output/20250208164938/chat_template.json
